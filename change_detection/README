This work requires two different data streams:
1) Initial - the initial recording that we will be creating a NeRF model from
2) Changed - the second recording that we are investigating to find/identify Changed

*** Step 1 - Create the file structure ***
In the original work, images were generated from a ROS bagfile using the associated rgbd_image_saver.py. Resulting files were then stored in the following directory structure:

initial -
   color - rgb_*.png
   depth - depth_*.png
   images.txt - the pose reported by the ros tf library
changed - 
   color - rgb_*.png
   depth - depth_*.png
   images.txt - the pose reported by the ros tf library

*** Step 2 - Train the NeRF model for the initial set of images ***
Create a conda environment, setup nerf

conda create -n nerfstudio3 python=3.8
conda activate nerfstudio3
pip install nerfstudio==1.1.0

Follow the instructions on the nerfstudio website to train a new model

ns-process-data images --data initial/color --output-dir initial/nerf_initial
ns-train nerfacto --data initial/nerf_initial

*** Step 3 - Prepare the directory for change estimation ***
Run the associated bash scripts to prepare the directory and register images

change_detection/bash/register_new_images.sh initial/nerf_initial depth

Note that this will try to rotate all of your color images by default since the images from the robot are rotated by 90 degrees.

Also note that the last two steps of this process are not automated. You will need to run them by hand as per the directions echoed to the terminal.

*** Step 4 - Create some images from the NeRF model***
python ~/ros_ws/src/research/change_detection/scripts/change_detection/render_transform.py outputs/nerf_no_person_initial/nerfacto/2024-05-21_204931/ /data2/datasets/office/no_person/monitor/transforms.json renders/monitor